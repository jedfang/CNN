{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=mnist.train.images.reshape(55000,28,28,1)\n",
    "y=mnist.train.labels.reshape(55000,10)\n",
    "x=x[:50,:,:]\n",
    "y=y[:50,:]\n",
    "print(x.shape)\n",
    "y=y.T\n",
    "m=y.shape[1]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchn_forward(A,gamma,beta,eps):\n",
    "    A_mu=np.mean(A,axis=0)\n",
    "    inv_var=1.0/(np.var(A,axis=0))\n",
    "    A_hat=(A-A_mu)/(np.sqrt(1.0/inv_var+eps))\n",
    "    z=gamma*A_hat+beta\n",
    "    cache2=A_mu, inv_var, A_hat, gamma\n",
    "    return z,cache2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchn_backward(dout, cache):\n",
    "    N, D = dout.shape\n",
    "    A_mu, inv_var, A_hat, gamma = cache\n",
    "    dAhat = dout * gamma\n",
    "    dA = (1.0 / N) * inv_var * (N*dAhat - np.sum(dAhat, axis=0) - A_hat*np.sum(dAhat*A_hat, axis=0))\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dgamma = np.sum(A_hat*dout, axis=0)\n",
    "\n",
    "    return dA, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    X_pad = np.pad(X,((0,0),(pad,pad),(pad,pad),(0,0)),'constant')\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    s =a_slice_prev*W\n",
    "    Z = np.sum(s)\n",
    "    Z = Z+np.float(b)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "     \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) =A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    n_H = int((n_H_prev-f+2*pad)/stride) +1\n",
    "    n_W = int((n_W_prev-f+2*pad)/stride) +1\n",
    "    Z = np.zeros((m,n_H,n_W,n_C))\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):                                \n",
    "        a_prev_pad = A_prev_pad[i]                         \n",
    "        for h in range(n_H):                          \n",
    "            for w in range(n_W):                       \n",
    "                for c in range(n_C):                   \n",
    "                     \n",
    "                    vert_start = h*stride\n",
    "                    vert_end = vert_start+f\n",
    "                    horiz_start = w*stride \n",
    "                    horiz_end = horiz_start+f\n",
    "                     \n",
    "                    a_slice_prev =a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "                    Z[i, h, w, c] =conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])\n",
    "                                        \n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "     \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"] \n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "\n",
    "    for i in range(m):                          \n",
    "        for h in range(n_H):                      \n",
    "            for w in range(n_W):                  \n",
    "                for c in range (n_C):             \n",
    "                     \n",
    "                    vert_start =h*stride\n",
    "                    vert_end = vert_start+f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = horiz_start+f\n",
    "                    \n",
    "                    a_prev_slice = A_prev[i,vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                     \n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "\n",
    "    cache = (A_prev, hparameters)\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(A_prev,w,b,gamma,beta,eps):\n",
    "    (m,hw) = A_prev.shape\n",
    "    Z=np.dot(w,A_prev)+b\n",
    "    ZT,cache2=batchn_forward(Z,gamma,beta,eps)\n",
    "    cache_fc=(A_prev,w,b)\n",
    "    return ZT,cache_fc,cache2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    Z_exp=np.exp(Z)\n",
    "    Z_sum=np.sum(Z_exp,axis=0)\n",
    "    Z_sf=Z_exp/Z_sum\n",
    "    return Z_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def loss(predicted,y):\n",
    "        m=y.shape[1]\n",
    "        loss=-np.log(predicted)*y\n",
    "        loss=(np.sum(loss))\n",
    "        loss=loss/m\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_backward(dZT,cache,cache2):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dZ, dgamma, dbeta=batchn_backward(dZT, cache2) \n",
    "    dW = 1.0/m*(np.dot(dZ,A_prev.T))\n",
    "    db = 1.0/m*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    return dA_prev,dW,db,dgamma,dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "     \n",
    "    mask = (x==np.max(x))\n",
    "    return mask\n",
    "\n",
    "def distribute_value(dz, shape):\n",
    "     \n",
    "    (n_H, n_W) = shape\n",
    "    average = n_H*n_W\n",
    "    a = np.ones(shape)*dz/average\n",
    "    return a\n",
    "\n",
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \n",
    "    (A_prev, hparameters) = cache \n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape \n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                        \n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):                   \n",
    "            for w in range(n_W):                \n",
    "                for c in range(n_C):            \n",
    "                    \n",
    "                    vert_start = h\n",
    "                    vert_end = h+f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = w+f\n",
    "                    \n",
    "                    if mode == \"max\": \n",
    "                        a_prev_slice = a_prev[vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask*dA[i,h,w, c]\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        da = dA[i,h,w,c]\n",
    "                        shape = (f,f)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
    "                        \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    return dA_prev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def conv_backward(dZ, cache):\n",
    "     \n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    dA_prev = np.zeros(A_prev.shape)                           \n",
    "    dW = np.zeros(W.shape)\n",
    "    db = np.zeros(b.shape)\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                         \n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        for h in range(n_H):                    \n",
    "            for w in range(n_W):                \n",
    "                for c in range(n_C):            \n",
    "                     \n",
    "                    vert_start = h*pad\n",
    "                    vert_end = h*pad+f\n",
    "                    horiz_start = w*pad\n",
    "                    horiz_end = w*pad+f\n",
    "                     \n",
    "                    a_slice = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:] \n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_0=x\n",
    "W_1=np.random.randn(3,3,1,3)*0.01\n",
    "b_1=np.random.randn(1,1,1,3)\n",
    "A_1,cache_1=conv_forward(A_0, W_1, b_1, hparameters={\"stride\":1,\"pad\":1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_2, cache_2= pool_forward(A_1, hparameters={\"f\":2,\"stride\":2}, mode = \"max\")\n",
    "A_2=(A_2.reshape(50,588)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_3=np.random.randn(10,588)*0.01\n",
    "b_3=np.random.randn(10,1)\n",
    "gamma=np.random.rand(10,1)\n",
    "beta=np.random.rand(10,1)\n",
    "eps=.0001\n",
    "ZT_3,cache_3,cache2=fc(A_2, W_3, b_3,gamma,beta,eps)\n",
    "A_3=softmax(ZT_3)\n",
    "dZT_3=A_3-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs.append(loss(A_3, y))\n",
    "temp_cost=loss(A_3, y)\n",
    "change=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    dA_2,dW_3,db_3,dgamma_3,dbeta_3=fc_backward(dZT_3, cache_3,cache2)\n",
    "    \n",
    "    dA_2=dA_2.T\n",
    "    dA_2=dA_2.reshape(50,14,14,3)\n",
    "    dA_1=pool_backward(dA_2, cache_2, mode = \"max\")\n",
    "    dZ_1=dA_1\n",
    "    dA_1,dW_1,db_1=conv_backward(dZ_1, cache_1)\n",
    "    learning_rate=0.0001\n",
    "    W_1=W_1-learning_rate*dW_1\n",
    "    W_3=W_3-learning_rate*dW_3\n",
    "    \n",
    "    b_1=b_1-learning_rate*db_1\n",
    "    b_3=b_3-learning_rate*db_3\n",
    "    gamma=gamma-learning_rate*dgamma_3\n",
    "    beta=beta-learning_rate*dbeta_3\n",
    "    A_1,cache_1=conv_forward(A_0, W_1, b_1, hparameters={\"stride\":1,\"pad\":1})\n",
    "    A_2, cache_2= pool_forward(A_1, hparameters={\"f\":2,\"stride\":2}, mode = \"max\")\n",
    "    A_2=(A_2.reshape(50,588)).T\n",
    "    Z_3,cache_3,cache2=fc(A_2, W_3, b_3,gamma,beta,eps)\n",
    "    A_3=softmax(Z_3)\n",
    "    cost=loss(A_3, y)\n",
    "    costs.append(cost)\n",
    "    change=temp_cost-cost\n",
    "    temp_cost=cost\n",
    "    \n",
    "    print(\"{} epoch completed and cost is {}\".format(i+1,cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
